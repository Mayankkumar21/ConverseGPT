# ConverseGPT

**_Live screenshots_**
![Demo](https://private-user-images.githubusercontent.com/95433204/318617715-07c757e2-3f25-4843-a1aa-dbd184f8ba3b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTIwMjQxNzAsIm5iZiI6MTcxMjAyMzg3MCwicGF0aCI6Ii85NTQzMzIwNC8zMTg2MTc3MTUtMDdjNzU3ZTItM2YyNS00ODQzLWExYWEtZGJkMTg0ZjhiYTNiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDAyVDAyMTExMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJhN2NmYjEwMDNiOGFjZjgxMTg3NTk3MTBiMzcxMjJjMTlkYzkzODhjYThlZmFiNjYxMzdiNjFkZmYyYmFhM2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.02Qlg6zv4j7RLTwCKi2ZxxzudLWbehAOqwijd7PZvGs)

**Setting up the project**
**_Installing Ollama_**
Download and install Ollama from https://ollama.com/
Select a model you want to run locally
Lets run llama2 model as example

```
ollama run llama2
```

During first time, it will pull the model
Now you can successfully use your own running LLMs question in the terminal

**_Running the project_**

```
https://github.com/Mayankkumar21/ConverseGPT.git
```

Install dependencies

```
pip install -r requirements.txt
```

**_Run the project_**

```
python main.py
```
